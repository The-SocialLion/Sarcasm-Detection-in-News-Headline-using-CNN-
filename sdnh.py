# -*- coding: utf-8 -*-
"""SDNH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cl2tkWevRRFJF-XQmu77T_WMVxpawdl2

#**Sarcasm Detection using Multi Channel CNN In News Headlines**
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import nltk
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from sklearn.metrics import confusion_matrix, f1_score
from keras import models, layers, optimizers, losses, callbacks
from keras.utils.vis_utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers.merge import concatenate
import zipfile

zip=zipfile.ZipFile('archive (4).zip')
zip.extractall()

"""**Processing Data**"""

df1=pd.read_json('Sarcasm_Headlines_Dataset.json',lines = True)
df2=pd.read_json('Sarcasm_Headlines_Dataset_v2.json',lines = True)

df1

df2

frames=[df1,df2]
df=pd.concat(frames)

df.reset_index(drop=True, inplace=True)
df

df.iloc[-1,:].values

df['is_sarcastic'].value_counts()

"""**Data preprocessing**"""

# Remove html tags
def removeHTML(sentence):
    regex = re.compile('<.*?>')
    return re.sub(regex, ' ', sentence)

# Remove URLs
def removeURL(sentence):
    regex = re.compile('http[s]?://\S+')
    return re.sub(regex, ' ', sentence)

# remove numbers, punctuation and any special characters (keep only alphabets)
def onlyAlphabets(sentence):
    regex = re.compile('[^a-zA-Z]')
    return re.sub(regex, ' ', sentence)

sno = nltk.stem.SnowballStemmer('english')    # Initializing stemmer
wordcloud = [[], []]
all_sentences = []    # All cleaned sentences


for x in range(len(df['headline'].values)):
    headline = df['headline'].values[x]
    sarcasm = df['is_sarcastic'].values[x]

    cleaned_sentence = []
    sentence = removeURL(headline) 
    sentence = removeHTML(sentence)
    sentence = onlyAlphabets(sentence)
    sentence = sentence.lower()   

    for word in sentence.split():
        #if word not in stop:
            stemmed = sno.stem(word)
            cleaned_sentence.append(stemmed)
            
            wordcloud[sarcasm].append(word)
            

    all_sentences.append(' '.join(cleaned_sentence))

# add as column in dataframe
X = all_sentences
y = df['is_sarcastic']

class_names = ['Not sarcastic', 'Sarcastic']

"""**Word cloud**"""

plt.figure(figsize=(10,10))

for i in range(len(class_names)):
    ax = plt.subplot(len(class_names), 1, i + 1)
    plt.imshow(WordCloud().generate(' '.join(wordcloud[i])))
    plt.title(class_names[i])
    plt.axis("off")

"""**Data split**"""

# Splitting into train and val set -- 80-20 split

Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2)

# Tokenization
vocab = 1500
mlen = 200
 
tokenizer = Tokenizer(num_words = vocab, oov_token = '<UNK>')
tokenizer.fit_on_texts(Xtrain)
 
Xtrain = tokenizer.texts_to_sequences(Xtrain)
Xtrain = pad_sequences(Xtrain, maxlen=mlen)

Xval = tokenizer.texts_to_sequences(Xval)
Xval = pad_sequences(Xval, maxlen=mlen)

Xtrain.shape

Xval.shape

"""**Multi channel Model using CNN Construction and Evaluation**"""

#channel 1
inputs1 = Input(shape=(mlen,))
embedding1 = Embedding(vocab, 100)(inputs1)
conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)
drop1 = Dropout(0.5)(conv1)
pool1 = MaxPooling1D(pool_size=2)(drop1)
flat1 = Flatten()(pool1)
# channel 2
inputs2 = Input(shape=(mlen,))
embedding2 = Embedding(vocab, 100)(inputs2)
conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)
drop2 = Dropout(0.5)(conv2)
pool2 = MaxPooling1D(pool_size=2)(drop2)
flat2 = Flatten()(pool2)
# channel 3
inputs3 = Input(shape=(mlen,))
embedding3 = Embedding(vocab, 100)(inputs3)
conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)
drop3 = Dropout(0.5)(conv3)
pool3 = MaxPooling1D(pool_size=2)(drop3)
flat3 = Flatten()(pool3)
# merge
merged = concatenate([flat1, flat2, flat3])
# interpretation
dense1 = Dense(10, activation='relu')(merged)
outputs = Dense(1, activation='sigmoid')(dense1)
model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)
# compile
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# summarize
model.summary()
plot_model(model, show_shapes=True, to_file='multichannel.png')

history=model.fit([Xtrain,Xtrain,Xtrain], ytrain, epochs=25, batch_size=2500,validation_data=([Xval,Xval,Xval],yval))
model.save('SDNH.h5')

model.evaluate([Xval,Xval,Xval] ,yval)

"""**Confusion Matrix**"""

cm = confusion_matrix(yval, (model.predict([Xval,Xval,Xval])>0.5).astype('int64'))
cm = cm.astype('int') / cm.sum(axis=1)[:, np.newaxis]

fig = plt.figure(figsize = (5, 5))
ax = fig.add_subplot(111)

for i in range(cm.shape[1]):
    for j in range(cm.shape[0]):
        if cm[i,j] > 0.8:
            clr = "white"
        else:
            clr = "black"
        ax.text(j, i, format(cm[i, j], '.2f'), horizontalalignment="center", color=clr)

_ = ax.imshow(cm, cmap=plt.cm.Blues)
ax.set_xticks(range(len(class_names)))
ax.set_yticks(range(len(class_names)))
ax.set_xticklabels(class_names, rotation = 90)
ax.set_yticklabels(class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""**Plotting Accuracy and Loss**"""

def plot(history, variable, variable2):
    plt.plot(range(len(history[variable])), history[variable])
    plt.plot(range(len(history[variable2])), history[variable2])
    plt.legend([variable, variable2])
    plt.title(variable)

plot(history.history, "accuracy", 'val_accuracy')

plot(history.history, "loss", 'val_loss')

"""**Model prediction**"""

x = np.random.randint(0, Xval.shape[0] - 1)

headline = df['headline'].values[x]

print("Headline: ", headline)

cleaned_text = []

sentence = removeURL(headline) 
sentence = removeHTML(sentence)
sentence = onlyAlphabets(sentence)
sentence = sentence.lower()   

for word in sentence.split():
    #if word not in stop:
        stemmed = sno.stem(word)
        cleaned_text.append(stemmed)

cleaned_text = [' '.join(cleaned_text)]

print("Cleaned text: ", cleaned_text[0])

cleaned_text = tokenizer.texts_to_sequences(cleaned_text)
cleaned_text = pad_sequences(cleaned_text, maxlen=mlen)

category = df['is_sarcastic'].values[x]  

print("\nTrue category: ", class_names[category])
m=cleaned_text
output = model.predict([m,m,m])
pred=0
pred = int(np.round(output,0))
#print(pred)
print("\nPredicted category: ",class_names[pred],"(", output, "-->", pred, ")")